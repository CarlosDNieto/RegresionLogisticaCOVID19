{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from requests.exceptions import ConnectionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo 4\n",
    "\n",
    "Igual que el 3 pero con la covariable **NEUMONIA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"COVID\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtraPositivos(df):\n",
    "    \"\"\"\n",
    "    Esta función filtra los casos positivos de un DataFrame dado.\n",
    "    df: spark DataFrame con la columna 'resultado'\n",
    "    \"\"\"\n",
    "    #Devolvemos nuestro df con los casos positivos\n",
    "    return df.filter(df.RESULTADO == 'Positivo SARS-CoV-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaNulls(df):\n",
    "    \"\"\"\n",
    "    Función que limpia nulls en un DF de spark e imprime\n",
    "    la cantidad de registros que borró (cantidad de valores\n",
    "    nulos).\n",
    "    \"\"\"\n",
    "    cleanDF = df.na.drop()\n",
    "    \n",
    "    print(\"Numero de registros con algun valor nulo: \", df.count() - cleanDF.count())\n",
    "    return cleanDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path del archivo csv\n",
    "file_local_path = 'C:/Users/nieto/Desktop/casos-asociados-a-covid-19.csv'\n",
    "\n",
    "#cargamos el archivo en un spark DataFrame\n",
    "df_origen = spark.read.csv(file_local_path, header=True, encoding='UTF-8', inferSchema=True)\n",
    "\n",
    "#covariables que vamos a tomar en cuenta para el modelo\n",
    "features = [\"TIPO PACIENTE\", \"SEXO\", \"EDAD\", \"EMBARAZO\", \"DIABETES\", \"EPOC\", \"ASMA\", \"INMUNOSUPRESION\", \"HIPERTENSION\",\n",
    "            \"OTRA COMPLICACION\", \"CARDIOVASCULAR\", \"OBESIDAD\", \"RENAL CRONICA\", \"TABAQUISMO\",\"RESULTADO\",\"NEUMONIA\"]\n",
    "df = df_origen.select(features)\n",
    "\n",
    "#filtramos los casos positivos\n",
    "df = filtraPositivos(df)\n",
    "#ya no necesitamos la columna resultado\n",
    "df = df.drop('RESULTADO')\n",
    "#limpiamos nuestra información\n",
    "df = limpiaNulls(df)\n",
    "#creamos una vista temporal en spark\n",
    "df.createOrReplaceTempView('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos lo que necesitamos de spark.ml\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler, MinMaxScaler, ChiSqSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procesamiento de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessing(train, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\"):\n",
    "    \"\"\"Función que hace todo el preprocesamiento de los datos\n",
    "    categóricos de un conjunto de datos de entrenamiento (o no).\n",
    "    :param train spark df: conjunto de datos de entrenamiento.\n",
    "    :param categoricalCols list,array: conjunto de nombres de columnas categoricas del\n",
    "        conjunto de datos train.\n",
    "    :param numericalCols list,array: conjunto de nombres de columnas numéricas del \n",
    "        conjunto de datos train.\n",
    "    :param labelCol str: variable objetivo o etiqueta\n",
    "    \n",
    "    :Returns spark dataframe con las columnas 'label' y 'features'\n",
    "    \"\"\"\n",
    "    \n",
    "    #codificamos todas las variables categóricas\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalCols:\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol= categoricalCol + \"Index\")\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                        outputCols=[categoricalCol + \"ohe\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "    #variable objetivo (etiqueta)\n",
    "    label_strIdx = StringIndexer(inputCol=labelCol, outputCol=\"label\")\n",
    "    stages += [label_strIdx]\n",
    "\n",
    "    #ponemos todas las covariables en un vector\n",
    "    assemblerInputs = [c + \"ohe\" for c in categoricalCols] + numericalCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"feat\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    #seleccionamos las variables que nos sirven con ChiSqSelector\n",
    "    selector = ChiSqSelector(featuresCol=\"feat\", outputCol=\"feature\", labelCol=\"label\", fpr=0.05,\n",
    "                            selectorType='fpr')\n",
    "    stages += [selector]\n",
    "    \n",
    "    #escala de 0-1\n",
    "    scala = MinMaxScaler(inputCol=\"feature\", outputCol=\"features\")\n",
    "    stages += [scala]\n",
    "    \n",
    "    #pipeline donde vamos a hacer todo el proceso\n",
    "    pipe = Pipeline(stages=stages)\n",
    "    pipeModel = pipe.fit(train)\n",
    "    train = pipeModel.transform(train)\n",
    "    \n",
    "    #regresamos nuestro df con lo que necesitamos\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parámetros necesarios para procesamiento de datos\n",
    "categoricalCols = list(df.columns[1:])\n",
    "categoricalCols.remove(\"EDAD\")\n",
    "numericalCols = [\"EDAD\"]\n",
    "\n",
    "#con dataProcessing() procesamos toda nuestra data\n",
    "raw_data = dataProcessing(df, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separamos nuestros datos en conjunto de entrenamiento y prueba\n",
    "train, test = raw_data.randomSplit([0.70,0.30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.select(\"label\",\"features\").show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numHosp = train.filter(df[\"TIPO PACIENTE\"]==\"HOSPITALIZADO\").count()\n",
    "numAmb = train.filter(df[\"TIPO PACIENTE\"]==\"AMBULATORIO\").count()\n",
    "\n",
    "#BalancingRatio nos ayuda a darle más peso a la clase minoritaria\n",
    "BalancingRatio = numAmb / (numHosp + numAmb)\n",
    "print(\"Balancing Ratio: \", BalancingRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "train=train.withColumn(\"classWeights\", when(train.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "train.select(\"classWeights\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeloLogistico(data, labelCol=\"label\", featuresCol=\"features\", weightCol=\"\"):\n",
    "    \"\"\"\n",
    "    Función que se encarga de ajustar un modelo logístico\n",
    "    a partir de un dataframe de spark con el esquema ya procesado\n",
    "    a partir de la función dataProcessing().\n",
    "    \n",
    "    :param data: spark dataframe.\n",
    "    :param labelCol: string nombre de la columna con la variable respuesta.\n",
    "    :param featuresCol: string nombre de la columna con los vectores de las\n",
    "        covariables.\n",
    "    \n",
    "    :returns modelo ajustado:\n",
    "    \"\"\"\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "    model = LogisticRegression(featuresCol=featuresCol, labelCol=labelCol, weightCol=weightCol)\n",
    "    return model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos nuestro modelo con el conjunto de entrenamiento y la función modeloLogistico()\n",
    "model = modeloLogistico(data=train, labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coeficientes: \",str(model.coefficientMatrix))\n",
    "print(\"Intercepto: \", str(model.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSummary = model.summary\n",
    "\n",
    "roc = modelSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "print('Training set areaUnderROC: ' + str(modelSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Presición vs Exhaustividad (recall)\n",
    "pr = modelSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLogistico(test,categoricalCols, numericalCols, labelCol, model):\n",
    "#     dataProcessing(train, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\")\n",
    "    \"\"\"\n",
    "    Esta función predice un modelo logístico con columnas categóricas\n",
    "    y numéricas sobre un conjunto de datos de prueba.\n",
    "    \"\"\"\n",
    "    predictions = model.transform(test)\n",
    "    #regresamos el df con las predicciones\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hacemos nuestras predicciones aplicándole el modelo a nuestro conjunto de prueba\n",
    "pred = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matriz de confusión\n",
    "#índice: etiqueta real\n",
    "#columnas: predicción de nuestro modelo\n",
    "pred.groupby(\"label\").pivot(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set Area Under ROC: 0.7633663135741994\n",
      "Test set Area Under ROC 0.7645725377264613\n"
     ]
    }
   ],
   "source": [
    "#AUROC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Training set Area Under ROC: ' + str(modelSummary.areaUnderROC))\n",
    "print('Test set Area Under ROC', evaluator.evaluate(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión:\n",
    "\n",
    "Este modelo no elimina ninguna varibale y se termina comportando igual que el modelo en el notebook principal. Así que para quitarnos de duda, haré un Modelo 3 haciendo el procesamiento del pipeline como Jimmy lo hace en una captura de pantalla sin el OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
