{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Regresión Logística\n",
    "\n",
    "---\n",
    "\n",
    "#### Carlos David Nieto Loya\n",
    "\n",
    "#### Tarea 4\n",
    "\n",
    "* **Objetivo:** Predecir -a partir de sus características físicas y médicas- si una persona infectada de COVID-19 tiene probabilidades altas de ser internada en un hospital a causa de que la enfermedad se complicó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from requests.exceptions import ConnectionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "    from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"COVID\") \\\n",
    "        .config(\"hive.exec.dynamic.partition\", \"true\")\\\n",
    "        .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurado para traer 10000 rows\n",
    "# url = 'https://datos.cdmx.gob.mx/api/records/1.0/search/?dataset=casos-asociados-a-covid-19&q=&rows=10000&facet=fecha_actualizacion&facet=origen&facet=sector&facet=entidad_um&facet=sexo&facet=entidad_nac&facet=entidad_res&facet=municipio_res&facet=tipo_paciente&facet=fecha_ingreso&facet=fecha_def&facet=edad&facet=nacionalidad&facet=embarazo&facet=habla_lengua_indi&facet=diabetes&facet=epoc&facet=asma&facet=inmusupr&facet=hipertension&facet=cardiovascular&facet=obesidad&facet=renal_cronica&facet=tabaquismo&facet=resultado&facet=migrante&facet=pais_nacionalidad&facet=rango_edad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getData(url):\n",
    "#     \"\"\"\n",
    "#     Funcion de obtiene la data de la API REST\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         resp = requests.get(url)\n",
    "#         if resp.status_code != 200:\n",
    "#             print(\"Pagina no encontrada se produjo el error \" + str(resp.status_code))\n",
    "#             return None\n",
    "#         else:\n",
    "#             print(\"El numero de hits(registros) es\",resp.json()[\"nhits\"])\n",
    "#             rdd=sc.parallelize(resp.json()[\"records\"])\n",
    "#             return rdd\n",
    "#     except ConnectionError as e:    \n",
    "#            print(\"Se produjo un error de conexión revise su url\\n\" +str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1 \n",
    "1. La funcion deberá recibir el rdd de arriba y devolver un dataframe, haga las validaciones necesarias con excepciones ya que el rdd puede ser vacío.    \n",
    "    \n",
    "    1. Investigue y explique que es una API REST\n",
    "    2. No deberá incluir ninguna acción, únicamente una transformación no print's.\n",
    "    3. Deberan comentar todo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta A:** \n",
    "\n",
    "Una API REST es un servicio web que sigue una serie de [métodos](https://www.restapitutorial.com/lessons/httpmethods.html) (POST, GET, PUT, PATCH, DELETE) del protocolo HTTP para la transferencia de datos. Normalmente estas API REST devuelven los datos en XML o en JSON.\n",
    "\n",
    "\n",
    "En la actualidad se prefiere enviar los datos en JSON ya que son archivos más ligeros.\n",
    "\n",
    "\n",
    "Es un backend que tiene una serie de urls en el que hacemos una serie de peticiones, y ésta nos devuelve los datos que solicitamos. Es una herramienta muy potente en la actualidad para el desarrollo de aplicaciones por el lado de servidores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rddtoDF(rdd,features):\n",
    "#     \"\"\"\n",
    "#     Esta funcion transforma el rdd en un dtaframe conservando los features que recibe como paraetros \n",
    "#     :param rdd: un rdd con los datos de la api\n",
    "#     :param features:  los covariables que deseo conservar\n",
    "#     :return: df, regresa un dataframe con la información\n",
    "#     \"\"\"\n",
    "#     #Variable temporal con los datos que necesitamos\n",
    "#     temp = rdd.map(lambda row: row['fields'])\n",
    "    \n",
    "#     #Transformamos el RDD en un DataFrame\n",
    "#     df = temp.toDF()\n",
    "    \n",
    "#     #Seleccionamos unicamente las covariables que se desean conservar\n",
    "#     df = df.select(features)\n",
    "    \n",
    "#     #Regresamos el DataFrame resultado\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = getData(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2 \n",
    "\n",
    "2. La función deberá recibir el dataframe de arriba y guardar la información en una tabla en HIVE en hdfs, para fines de tener un historico.\n",
    "\n",
    "    1. El formato de guardado de datos de la tabla Hive debe ser un parquet file.\n",
    "    2. La tabla debe estar particionada por fecha.\n",
    "    3. Suponga que el proceso se correrá todos los dias, así que deben contemplar que una vez que corra la    primera vez el proceso, para los dias siguientes únicamente se deberá almacenar los datos de ese día.\n",
    "    4. El modo es un parametro que indica si desea reescribir los datos o hacer un append, para pruebas use el   modo overwrite.\n",
    "    5. Cuando su modelo funcione correctamente debera cambiar el número de rows por el número de Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #funcion axiliar por si la necesita\n",
    "# def recorreVentana(fecha):\n",
    "#     #insertar codigo\n",
    "#     return None   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardardatosenHive(df,modo,table=\"covid.casoscovid\"):\n",
    "    #insertar codigo\n",
    "#     df.write.partitionBy(\"particion\").mode(\"append\").format(\"parquet\").saveAsTable(tableNameResult)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3\n",
    "3. Implemente una función que filtre aquellos registros que dieron positivo para sars-covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtraPositivos(df):\n",
    "    \"\"\"\n",
    "    Esta función filtra los casos positivos de un DataFrame dado.\n",
    "    df: spark DataFrame con la columna 'resultado'\n",
    "    \"\"\"\n",
    "    #Devolvemos nuestro df con los casos positivos\n",
    "    return df.filter(df.RESULTADO == 'Positivo SARS-CoV-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "4. Implemente una función que filtre los nulls si es que existen. Cuente el número de registros nulos en cada campo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiaNulls(df):\n",
    "    \"\"\"\n",
    "    Función que limpia nulls en un DF de spark e imprime\n",
    "    la cantidad de registros que borró (cantidad de valores\n",
    "    nulos).\n",
    "    \"\"\"\n",
    "    cleanDF = df.na.drop()\n",
    "\n",
    "    print(\"Numero de registros con algun valor nulo: \", df.count() - cleanDF.count())\n",
    "    return cleanDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #características físcias de las personas que pueden\n",
    "# #ayudar a que se hospitalice y las fechas para\n",
    "# #graficar las series de tiempo\n",
    "# features = ['inmusupr', 'tipo_paciente', 'cardiovascular','neumonia', \n",
    "#             'hipertension', 'otra_com', 'epoc', 'asma', 'tabaquismo',\n",
    "#             'embarazo', 'sexo', 'obesidad', 'fecha_sintomas',\n",
    "#             'diabetes', 'edad', 'fecha_ingreso', 'renal_cronica','resultado']\n",
    "\n",
    "# #transformamos nuestro RDD en un SparkDataFrame\n",
    "# df = rddtoDF(rdd, features)\n",
    "# #filtramos los casos positivos\n",
    "# df = filtraPositivos(df)\n",
    "# #limpiamos nuestra información\n",
    "# df = limpiaNulls(df)\n",
    "# #creamos una vista temporal en spark\n",
    "# df.createOrReplaceTempView('covid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path del archivo csv\n",
    "file_local_path = '/Users/carlosnieto/Desktop/casos-asociados-a-covid-19.csv'\n",
    "\n",
    "#cargamos el archivo en un spark Dat~Frame\n",
    "df_origen = spark.read.csv(file_local_path, header=True, encoding='UTF-8', inferSchema=True)\n",
    "\n",
    "#covariables que vamos a tomar en cuenta para el modelo\n",
    "features = [\"TIPO PACIENTE\", \"SEXO\", \"EDAD\", \"EMBARAZO\", \"DIABETES\", \"EPOC\", \"ASMA\", \"INMUNOSUPRESION\", \"HIPERTENSION\",\n",
    "            \"OTRA COMPLICACION\", \"CARDIOVASCULAR\", \"OBESIDAD\", \"RENAL CRONICA\", \"TABAQUISMO\",\"RESULTADO\"]\n",
    "df = df_origen.select(features)\n",
    "#filtramos los casos positivos\n",
    "df = filtraPositivos(df)\n",
    "#ya no necesitamos la columna resultado\n",
    "df = df.drop('RESULTADO')\n",
    "#limpiamos nuestra información\n",
    "df = limpiaNulls(df)\n",
    "#creamos una vista temporal en spark\n",
    "df.createOrReplaceTempView('covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5\n",
    "5. Responda lo siguiente:    \n",
    "\n",
    "    2. Instale la siguiente biblioteca pyarrow para agilizar el intercambio entre pandas y spark.\n",
    "    3. ¿Cuál es la media de las edades de las personas contagiadas?. Haga un histograma\n",
    "    4. ¿Se tienen datos balaceados?. Investigue técnicas para datos desbalanceados.\n",
    "    5. Haga tablas cruzadas de tipo_paciente con obesidad, diabetes, epoc, etc. Explique lo que observa.\n",
    "    8. Haga un agrupamiento de casos y decesos por estado y grafique (plotly) las series de tiempo con los 10 estados con más casos.\n",
    "    \n",
    "***\n",
    "    Puede usar Pandas para la generación de la tablas cruzadas.\n",
    "     Use la sig intrucción para instalar la biblioteca\n",
    "     conda install -c conda-forge pyarrow\n",
    "     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Edades de las personas contagiadas\n",
    "\n",
    "Aquí andalizamos las edades de las personas contagiadas con histogramas y boxplots para entender mejor a que sector de la sociedad por edad se ve mas afectada por esta enfermedad.\n",
    "\n",
    "\n",
    "En este análisis llegamos a concluir lo siguiente:\n",
    "\n",
    "* La media de las edades de las personas contagiadas es de: **46.2 años**.\n",
    "\n",
    "\n",
    "* La mayoría de las personas contagiadas se encuentra entre los 40 y 50 años.\n",
    "\n",
    "\n",
    "* Tenemos observaciones de hasta 120 años de edad (outliers).\n",
    "\n",
    "\n",
    "* La edad promedio de un hospitalizado es de: **54.7 años**\n",
    "\n",
    "\n",
    "* La edad promedio de los casos ambulatorios es de: **41.7 años**\n",
    "\n",
    "\n",
    "* Las personas hospitalizadas son mayores (en general) que las personas que no fueron hospitalizadas.\n",
    "\n",
    "\n",
    "> De esto podemos concluir que la edad es una característica física que nos puede ayudar a asignar una probabilidad de ser hospitalizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#iniciaizamos nuestras gráficas\n",
    "f,axes = plt.subplots(figsize=(15,6), ncols=2)\n",
    "#extraemos las edades en un array de python\n",
    "edad = df.select(\"EDAD\").toPandas()[\"EDAD\"].values\n",
    "\n",
    "#primero graficamos el histograma\n",
    "sns.distplot(edad, ax=axes[0], bins=10, kde = False, norm_hist=True,\n",
    "             hist_kws={\n",
    "                 'edgecolor':'black',\n",
    "                 'color':'steelblue',\n",
    "                 'linewidth':1\n",
    "             })\n",
    "#añadimos una linea vertical señalando el promedio\n",
    "prom = edad.mean()\n",
    "axes[0].axvline(prom, 0,1, color='red', linestyle='--')\n",
    "#anotamos el promedio en el histograma\n",
    "text = 'Promedio: {:.1f}'.format(prom)\n",
    "axes[0].annotate(text, xy=(prom+3,0.965), xycoords=('data', 'axes fraction'), color='red')\n",
    "\n",
    "#graficamos el boxplot\n",
    "sns.boxplot(edad, orient='v', ax=axes[1], color='lightblue')\n",
    "\n",
    "#titulos para el histograma y el boxplot\n",
    "axes[0].title.set_text('Figura 1.0: Histograma de casos de COVID-19 en México por edad')\n",
    "axes[1].title.set_text('Figura 1.1: Boxplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[df[\"TIPO PACIENTE\"] == 'HOSPITALIZADO'].select(\"EDAD\")\n",
    "edades = temp.toPandas()[\"EDAD\"].values\n",
    "\n",
    "#iniciaizamos nuestras gráficas\n",
    "f,axes = plt.subplots(figsize=(15,6), ncols=2)\n",
    "\n",
    "#primero graficamos el histograma\n",
    "sns.distplot(edades, ax=axes[0], bins=10, kde = False, norm_hist=True,\n",
    "             hist_kws={\n",
    "                 'edgecolor':'black',\n",
    "                 'color':'steelblue',\n",
    "                 'linewidth':1\n",
    "             })\n",
    "#añadimos una linea vertical señalando el promedio\n",
    "prom = edades.mean()\n",
    "axes[0].axvline(prom, 0,1, color='red', linestyle='--')\n",
    "#anotamos el promedio en el histograma\n",
    "text = 'Promedio: {:.1f}'.format(prom)\n",
    "axes[0].annotate(text, xy=(prom+3,0.965), xycoords=('data', 'axes fraction'), color='red')\n",
    "\n",
    "#graficamos el boxplot\n",
    "sns.boxplot(edades, orient='v', ax=axes[1], color='lightblue')\n",
    "\n",
    "#titulos para el histograma y el boxplot\n",
    "axes[0].title.set_text('Figura 2.0: HOSPITALIZADOS en México por COVID-19 por edad')\n",
    "axes[1].title.set_text('Figura 2.1: HOSPITALIZADOS Boxplot')\n",
    "\n",
    "f.show()\n",
    "\n",
    "temp = df[df[\"TIPO PACIENTE\"] == 'AMBULATORIO'].select(\"EDAD\")\n",
    "edades = temp.toPandas()[\"EDAD\"].values\n",
    "\n",
    "#iniciaizamos nuestras gráficas\n",
    "g,axes = plt.subplots(figsize=(15,6), ncols=2)\n",
    "\n",
    "\n",
    "#primero graficamos el histograma\n",
    "sns.distplot(edades, ax=axes[0], bins=10, kde = False, norm_hist=True,\n",
    "             hist_kws={\n",
    "                 'edgecolor':'black',\n",
    "                 'color':'steelblue',\n",
    "                 'linewidth':1\n",
    "             })\n",
    "#añadimos una linea vertical señalando el promedio\n",
    "prom = edades.mean()\n",
    "axes[0].axvline(prom, 0,1, color='red', linestyle='--')\n",
    "#anotamos el promedio en el histograma\n",
    "text = 'Promedio: {:.1f}'.format(prom)\n",
    "axes[0].annotate(text, xy=(prom+3,0.965), xycoords=('data', 'axes fraction'), color='red')\n",
    "\n",
    "#graficamos el boxplot\n",
    "sns.boxplot(edades, orient='v', ax=axes[1], color='lightblue')\n",
    "\n",
    "#titulos para el histograma y el boxplot\n",
    "axes[0].title.set_text('Figura 2.3: Casos AMBULATORIOS en México por COVID-19 por edad')\n",
    "axes[1].title.set_text('Figura 2.4: Casos AMBULATORIOS Boxplot')\n",
    "\n",
    "g.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. ¿Se tienen datos balanceados?\n",
    "\n",
    "Haciendo un **``groupby``** sobre la columna **``TIPO PACIENTE``** (la cual es nuestra variable objetivo en este caso) podemos observar que hay más casos ambulatorios que casos de hospitalización en México aunque la diferencia no es mucha, puede causar problemas al momento de ajustar el modelo. \n",
    "\n",
    "Algo bueno es que contamos con un método que se llama **\"Class Weighting\"**, en la regresión logística de spark, que nos sirve para poder darle más peso a la clase minoritaria, y así, hacer que el modelo no se vea afectado por el desbalance en los datos.\n",
    "\n",
    "Este método consiste en sacar el **``BalancingRatio``** de nuestros datos, el cual vamos a definir como:\n",
    "\n",
    "$$BalancingRatio = \\frac{numNegatives}{DatasetSize}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"TIPO PACIENTE\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Tablas cruzadas\n",
    "Haremos tablas y gráficas cruzadas para poder observar qué tanto influyen las covariables sobre nuestra variable objetivo (**``TIPO PACIENTE``**).\n",
    "\n",
    "\n",
    "Para esto primero haremos un **Pandas DataFrame** para poder manipular los datos en python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfp = spark.sql('select * from covid').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#En esta celda hacemos todas las tablas cruzadas con nuestras covariables y la variable respuesta\n",
    "#excepto con la edad porque esa ya la analizamos\n",
    "cols = list(df.columns)\n",
    "cols.remove(\"TIPO PACIENTE\")\n",
    "cols.remove(\"EDAD\")\n",
    "\n",
    "for col in cols:\n",
    "    print(col,\":\")\n",
    "    df.groupby(col).pivot(\"TIPO PACIENTE\").count().show()\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de covariables\n",
    "\n",
    "En este análisis haremos gráficas cruzadas para poder observar qué covariables de padecimientos o condiciones físicas podemos tomar en cuenta para predecir si una persona con COVID-19 puede ser hospitalizada o no.\n",
    "\n",
    "\n",
    "También vamos a analizar la tasa de hospitalización por cada padecimiento físico, por ejemplo: de las personas que dieron positivo a COVID-19 que tienen diabetes, ¿qué porcentaje de éstas fueron hospitalizadas?. Calcularemos los valores de la siguiente manera:\n",
    "\n",
    "\n",
    "$$\\text{Tasa de hospitalizacion} = \\frac{\\text{No. de personas con diabetes hospitalizadas}}{\\text{No. total de personas con diabetes}} = \\frac{n}{N}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficaPorTipoPaciente(data, feature):\n",
    "    \"\"\"\n",
    "    Función que hace una gráfica para analisar una característica dada\n",
    "    contra el tipo de paciente (HOSPITALIZADO, AMBULATORIO)\n",
    "    - feature: característica a analizar\n",
    "    - data: pandas DataFrame de donde sale la información\n",
    "    Devuelve una figura de matplotlib\n",
    "    \"\"\"\n",
    "    #inicializamos la gráfica y graficamos con seaborn\n",
    "    g,ax = plt.subplots()\n",
    "    sns.countplot(data=data, x=feature, hue='TIPO PACIENTE', ax=ax)\n",
    "    \n",
    "    #totales por valor de la característica dada\n",
    "    totales = [data[data[feature]==val][feature].count().sum() for val in data[feature].unique()]\n",
    "    #duplicamos la lista para anotar los % de hospitalizados y ambulatorios\n",
    "    #para cada valor de la característica dada\n",
    "    totales = totales * 2\n",
    "    \n",
    "    #anotamos los % mencionados arriba\n",
    "    for patch,i in zip(ax.patches,range(len(ax.patches))):\n",
    "        x = patch.get_x()\n",
    "        y = patch.get_height()\n",
    "        if totales[i] != 0: #checamos que el total no sea cero\n",
    "            text = '{}%'.format(round(100*y/totales[i],1))\n",
    "        else:\n",
    "            text = '0%'\n",
    "        ax.annotate(text, xy=(x+0.05,y), color='black')\n",
    "    \n",
    "    return g,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = spark.sql('select * from covid').toPandas()\n",
    "cols = dfp.columns.to_list()\n",
    "cols.remove('TIPO PACIENTE')\n",
    "cols.remove('EDAD')\n",
    "cols.remove('SEXO')\n",
    "porcentajes = []\n",
    "\n",
    "for feature in cols:\n",
    "    #numero total de casos que tienen caracteristica i\n",
    "    N = dfp[dfp[feature]=='SI'].count().sum()\n",
    "    #numero de hospitalizados dentro de los que tienen la característica i\n",
    "    n = dfp[(dfp[feature]=='SI') & (dfp['TIPO PACIENTE']=='HOSPITALIZADO')].count().sum()\n",
    "    #porcentaje de hospitalizados de la característica i\n",
    "    porc = n/N\n",
    "    #almacenamos el porcentaje\n",
    "    porcentajes.append(porc)\n",
    "\n",
    "print(\"Tabla de tasas de hospitalización por padecimiento\")\n",
    "tasas_df = pd.DataFrame({'Padecimiento':cols, 'Tasa':porcentajes})\n",
    "tasas_df = tasas_df.sort_values(by='Tasa', ascending=False, ignore_index=True)\n",
    "tasas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padecimientos = tasas_df.Padecimiento.values\n",
    "\n",
    "for col in padecimientos:\n",
    "    graficaPorTipoPaciente(data=dfp, feature=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F. Haga un agrupamiento de casos y decesos por estado y grafique (plotly) las series de tiempo con los 10 estados con más casos.\n",
    "\n",
    "Para hacer esto, necesitaremos un DataFrame con las siguientes covariables:\n",
    "\n",
    "* Entidad de residencia\n",
    "* Fecha de inicio de síntomas\n",
    "* Fecha de defunción\n",
    "* Resultado (para filtrar solamente los positivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe de spark con solo la información que necesitamos para graficar\n",
    "dff = df_origen.select('ENTIDAD RESIDENCIA', 'FECHA SINTOMAS', 'FECHA DEFUNCION', 'RESULTADO')\n",
    "dff = filtraPositivos(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego vamos a necesitar 3 funciones diferentes que nos van a ayudar a graficar la información en plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spark_dataframe_by_list(df, column_name, filter_list):\n",
    "    \"\"\"Regresa un subconjunto del df donde df[column_name] esta en filter_list\"\"\"\n",
    "    sparky = SparkSession.builder.getOrCreate()\n",
    "    filter_df = sparky.createDataFrame(filter_list, df.schema[column_name].dataType)\n",
    "    return df.join(filter_df, df[column_name] == filter_df[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopCrossTableByDate(df,states_col,date_col,top=10):\n",
    "    \"\"\"\n",
    "    Esta funcion genera la serie de tiempo del top por estado de residencia.\n",
    "    \n",
    "    :param df: dataframe de donde salen los datos\n",
    "    :param states_col: nombre de la columna donde se encuentran los estados\n",
    "    :param date_col: nombre de la columna donde se encuentran las fechas\n",
    "    :param top: señala el # del top que queremos.\n",
    "    :returns: Pandas DataFrame con la serie de tiempo del toplist\n",
    "    \"\"\" \n",
    "    #sacamos el # de casos por estado (top solamente)\n",
    "    top_df = df.groupby(states_col).count().orderBy('count', ascending=False).limit(top)\n",
    "    #lista de los top estados\n",
    "    top_estados = list(top_df.select(states_col).toPandas()[states_col])\n",
    "    #filtramos el df de la info solamente de los top estados\n",
    "    filtered_df = filter_spark_dataframe_by_list(df, states_col, top_estados)\n",
    "    #hacemos la tabla cruzada de fechas (indices) vs estados (columnas)\n",
    "    timeSeries = filtered_df.crosstab(date_col, states_col).toPandas()\n",
    "    #nueva columna que se genera sola\n",
    "    new_col = date_col + \"_\" + states_col\n",
    "    #borramos los nulls (para el caso de las muertes nos genera nulls)\n",
    "    timeSeries = timeSeries[timeSeries[new_col]!='null']\n",
    "    #la columna que contiene las fechas la convertimos en fecha de python\n",
    "    timeSeries[new_col] = pd.to_datetime(timeSeries[new_col])\n",
    "    #ordenamos por fecha más antigua a la más reciente\n",
    "    timeSeries = timeSeries.sort_values(by=new_col, ascending=True, ignore_index=True)\n",
    "    #colocamos a la columna de las fechas en el index\n",
    "    timeSeries = timeSeries.set_index(new_col)\n",
    "    return timeSeries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficaSeriesDeTiempo(dataframe, titulo):\n",
    "    \"\"\"\n",
    "    Esta función grafica las series de tiempo de los estados (columnas) de\n",
    "    un dataframe donde las fechas vienen en el index del dataframe.\n",
    "    :param dataframe: pandas dataframe con la información de las series de tiempo por estado.\n",
    "    :param titulo: string que contiene el título del gráfico.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    #las columnas del dataframe deben de ser los estados que vamos a graficar\n",
    "    for estado in dataframe.columns:\n",
    "        trace = go.Scatter(\n",
    "            x = dataframe.index, # fechas\n",
    "            y = dataframe[estado].values, # serie de tiempo de cada estado\n",
    "            name = estado,\n",
    "            opacity = 0.8\n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "\n",
    "    layout = dict(title=titulo)\n",
    "    fig.layout = layout\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casos_positivos_por_estado = getTopCrossTableByDate(dff,\"ENTIDAD RESIDENCIA\", \"FECHA SINTOMAS\")\n",
    "graficaSeriesDeTiempo(casos_positivos_por_estado.cumsum(),\n",
    "                      titulo=\"Casos positivos acumulados de los 10 estados con más casos en México\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muertes_por_estado = getTopCrossTableByDate(dff,\"ENTIDAD RESIDENCIA\", \"FECHA DEFUNCION\")\n",
    "graficaSeriesDeTiempo(muertes_por_estado.cumsum(),\n",
    "                      titulo=\"Muertes acumuladas de los 10 estados con más defunciones en México\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6\n",
    "6. Implemente la sig función para el preprocesamiento, deberá realizar lo siguiente:\n",
    "    1. Codifique sus varibles catégoricas a tipo númerico.\n",
    "    2. Aplique oneHotEnconder(codificación parcial) a sus variables, explique cual es la raazón de hacerlo.\n",
    "    3. Guarde en un vector sparse su salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import  StringIndexer, VectorAssembler, MinMaxScaler, ChiSqSelector, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessing(train, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\"):\n",
    "    \"\"\"Función que hace todo el preprocesamiento de los datos\n",
    "    categóricos de un conjunto de datos de entrenamiento (o no).\n",
    "    :param train spark df: conjunto de datos de entrenamiento.\n",
    "    :param categoricalCols list,array: conjunto de nombres de columnas categoricas del\n",
    "        conjunto de datos train.\n",
    "    :param numericalCols list,array: conjunto de nombres de columnas numéricas del \n",
    "        conjunto de datos train.\n",
    "    :param labelCol str: variable objetivo o etiqueta\n",
    "    \n",
    "    :Returns spark dataframe con las columnas 'label' y 'features'\n",
    "    \"\"\"\n",
    "    \n",
    "    #codificamos todas las variables categóricas\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalCols:\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol= categoricalCol + \"Index\")\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                        outputCols=[categoricalCol + \"ohe\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "    #variable objetivo (etiqueta)\n",
    "    label_strIdx = StringIndexer(inputCol=labelCol, outputCol=\"label\")\n",
    "    stages += [label_strIdx]\n",
    "\n",
    "    #ponemos todas las covariables en un vector\n",
    "    assemblerInputs = [c + \"ohe\" for c in categoricalCols] + numericalCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"feat\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    #escala de 0-1\n",
    "    scala = MinMaxScaler(inputCol=\"feat\", outputCol=\"features\")\n",
    "    stages += [scala]\n",
    "    \n",
    "    #pipeline donde vamos a hacer todo el proceso\n",
    "    pipe = Pipeline(stages=stages)\n",
    "    pipeModel = pipe.fit(train)\n",
    "    train = pipeModel.transform(train)\n",
    "    \n",
    "    #regresamos nuestro df con lo que necesitamos\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 7\n",
    "6. Implemente un modelo Logístico.\n",
    "    1. Aplique un Hold-out a sus datos con un 70-30 para validación y entrenamiento. \n",
    "    2. Que coeficientes obtuvo.\n",
    "    3. Con la muestra de validación, obtenga el accuracy.\n",
    "    4. Deberá devolver un dataframe con las probalidad P(Y=1) y la predicción.\n",
    "    5. Obtenga la curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeloLogistico(data, labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\"):\n",
    "    \"\"\"\n",
    "    Función que se encarga de ajustar un modelo logístico\n",
    "    a partir de un dataframe de spark con el esquema ya procesado\n",
    "    a partir de la función dataProcessing().\n",
    "    \n",
    "    :param data: spark dataframe.\n",
    "    :param labelCol: string nombre de la columna con la variable respuesta.\n",
    "    :param featuresCol: string nombre de la columna con los vectores de las\n",
    "        covariables.\n",
    "    \n",
    "    :returns modelo ajustado:\n",
    "    \"\"\"\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    \n",
    "    model = LogisticRegression(featuresCol=featuresCol, labelCol=labelCol, weightCol=weightCol)\n",
    "    return model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLogistico(test, model):\n",
    "    \"\"\"\n",
    "    Esta función predice un modelo logístico con columnas categóricas\n",
    "    y numéricas sobre un conjunto de datos de prueba.\n",
    "    \"\"\"\n",
    "    #predecimos el modelo\n",
    "    predictions = model.transform(test)\n",
    "    #regresamos el df con las predicciones\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero definimos nuestras columnas categóricas, numéricas y nuestra variable objetivo\n",
    "categoricalCols = ['SEXO','EMBARAZO','DIABETES','EPOC','ASMA','INMUNOSUPRESION','HIPERTENSION','OTRA COMPLICACION',\n",
    "                   'CARDIOVASCULAR','OBESIDAD','RENAL CRONICA','TABAQUISMO']\n",
    "numericalCols = [\"EDAD\"]\n",
    "labelCol = \"TIPO PACIENTE\"\n",
    "\n",
    "#Procesamos nuestros datos con la función dataProcessing()\n",
    "raw_data = dataProcessing(df, categoricalCols, numericalCols, labelCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conjunto de entrenamiento y de prueba\n",
    "train, test = raw_data.randomSplit([0.70,0.30])\n",
    "\n",
    "#obtenemos el balancing ratio\n",
    "numHosp = train.filter(train[\"TIPO PACIENTE\"]==\"HOSPITALIZADO\").count()\n",
    "numAmb = train.filter(train[\"TIPO PACIENTE\"]==\"AMBULATORIO\").count()\n",
    "BalancingRatio = numAmb / (numHosp + numAmb)\n",
    "print(\"Balancing Ratio: \", BalancingRatio)\n",
    "\n",
    "#agregamos una columna con el BalancingRatio respectivo para cada label\n",
    "train=train.withColumn(\"classWeights\", when(train.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "\n",
    "#modelo de regresión logística\n",
    "model = modeloLogistico(data=train, labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\")\n",
    "#imprimimos los coeficientes\n",
    "print(\"Coeficientes: \",str(model.coefficientMatrix))\n",
    "print(\"Intercepto: \", str(model.interceptVector))\n",
    "\n",
    "#predicciones con el conjunto de prueba\n",
    "predictions = predictLogistico(test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "modelSummary = model.summary\n",
    "\n",
    "roc = modelSummary.roc.toPandas()\n",
    "plt.plot(roc['FPR'],roc['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(modelSummary.areaUnderROC))\n",
    "#AUROC del test set\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = modelSummary.pr.toPandas()\n",
    "plt.plot(pr['recall'],pr['precision'])\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Matriz de confusión en las predicciones del test set\n",
    "predictions.crosstab(\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#otras métricas de evaluación\n",
    "print(\"clases:\", modelSummary.labels)\n",
    "print(\"f-measure\",modelSummary.fMeasureByLabel(beta=1.0))\n",
    "print(\"false-positive rate by label:\", modelSummary.falsePositiveRateByLabel)\n",
    "print(\"Precisión: \", modelSummary.precisionByLabel)\n",
    "print(\"Exhaustividad (recall): \", modelSummary.recallByLabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 8\n",
    "6. Pruebas de hipotesis.\n",
    "    1. Aplique la prueba de la $\\chi^2$ para la selección de las variables relevantes en su modelo. \n",
    "    2. Obtenga la probablidad de riesgo de un sujeto de prueba. Deberá crear un dataframe de spark con el Esquema bien definido.\n",
    "    3. Crear un job que pueda ser lanzado mediante spark-submit.\n",
    "    4. Persista el modelo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruebaChi(dataframe, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\"):\n",
    "    \"\"\"\n",
    "    Función que procesa los datos pero añadiéndo la Prueba Chi para seleccionar variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    #codificamos todas las variables categóricas\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalCols:\n",
    "        stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol= categoricalCol + \"Index\")\n",
    "        encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                        outputCols=[categoricalCol + \"ohe\"])\n",
    "        stages += [stringIndexer, encoder]\n",
    "\n",
    "    #variable objetivo (etiqueta)\n",
    "    label_strIdx = StringIndexer(inputCol=labelCol, outputCol=\"label\")\n",
    "    stages += [label_strIdx]\n",
    "\n",
    "    #ponemos todas las covariables en un vector\n",
    "    assemblerInputs = [c + \"ohe\" for c in categoricalCols] + numericalCols\n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"feat\")\n",
    "    stages += [assembler]\n",
    "    \n",
    "    #seleccionamos las variables que nos sirven con ChiSqSelector\n",
    "    selector = ChiSqSelector(featuresCol=\"feat\", outputCol=\"feature\", labelCol=\"label\", fpr=0.05,\n",
    "                            selectorType='fpr')\n",
    "    stages += [selector]\n",
    "    \n",
    "    #escala de 0-1\n",
    "    scala = MinMaxScaler(inputCol=\"feature\", outputCol=\"features\")\n",
    "    stages += [scala]\n",
    "    \n",
    "    #pipeline donde vamos a hacer todo el proceso\n",
    "    pipe = Pipeline(stages=stages)\n",
    "    pipeModel = pipe.fit(dataframe)\n",
    "    df = pipeModel.transform(dataframe)\n",
    "    \n",
    "    #regresamos nuestro df con lo que necesitamos\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba Chi\n",
    "\n",
    "Ahora haremos el modelo pero sobre los datos donde corremos la prueba chi para selección de variables. Luego compararemos la diferencia entre el modelo sin y con la prueba Chi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data2 = pruebaChi(df, categoricalCols, numericalCols, labelCol=\"TIPO PACIENTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conjunto de entrenamiento y de prueba\n",
    "train2, test2 = raw_data2.randomSplit([0.70,0.30])\n",
    "\n",
    "#obtenemos el balancing ratio\n",
    "numHosp = train2.filter(train[\"TIPO PACIENTE\"]==\"HOSPITALIZADO\").count()\n",
    "numAmb = train2.filter(train[\"TIPO PACIENTE\"]==\"AMBULATORIO\").count()\n",
    "BalancingRatio = numAmb / (numHosp + numAmb)\n",
    "print(\"Balancing Ratio: \", BalancingRatio)\n",
    "\n",
    "#agregamos una columna con el BalancingRatio respectivo para cada label\n",
    "train2=train2.withColumn(\"classWeights\", when(train2.label == 1,BalancingRatio).otherwise(1-BalancingRatio))\n",
    "\n",
    "#modelo de regresión logística\n",
    "model2 = modeloLogistico(data=train2, labelCol=\"label\", featuresCol=\"features\", weightCol=\"classWeights\")\n",
    "#imprimimos los coeficientes\n",
    "print(\"Coeficientes: \",str(model2.coefficientMatrix))\n",
    "print(\"Intercepto: \", str(model2.interceptVector))\n",
    "\n",
    "#predicciones con el conjunto de prueba\n",
    "predictions = predictLogistico(test2, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSummary2 = model2.summary\n",
    "\n",
    "roc2 = modelSummary2.roc.toPandas()\n",
    "plt.plot(roc2['FPR'],roc2['TPR'])\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('Training set areaUnderROC: ' + str(modelSummary2.areaUnderROC))\n",
    "#AUROC del test set\n",
    "evaluator2 = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator2.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que las AUROC de ambos modelos no cambia mucho, por lo que la prueba Chi en esta caso no \"mejora\" el modelo, sino lo deja igual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probSujetodeprueba(df):\n",
    "    #insertar codigo\n",
    "    return probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModeloPersistente(modelo):\n",
    "    #insertar codigo\n",
    "    return modeloserializado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
